from app.ai.nodes.optimizers.base import optimize_program
from app.ai.nodes.agents.mentor import MentorAgent
from dspy import Example


def main():
    trainset = [
        Example(
            question="How does Merge Sort work?",
            answer="Merge Sort is a divide-and-conquer algorithm that recursively sorts two halves of an array and then merges them. It works by: 1) Dividing the array into two halves until each subarray has one element, 2) Recursively sorting each half, 3) Merging the sorted halves back together by comparing elements and placing them in order. Time complexity is O(n log n) in all cases, making it stable and predictable. The algorithm requires O(n) extra space for the temporary arrays used during merging. It's particularly useful for large datasets and when stability (maintaining relative order of equal elements) is important."
        ),

        Example(
            question="Explain the transformer architecture and why it revolutionized NLP",
            answer="The Transformer architecture, introduced in 'Attention is All You Need' (2017), revolutionized NLP by replacing recurrent and convolutional layers with self-attention mechanisms. Key components include: 1) Multi-head self-attention that allows the model to focus on different parts of the input simultaneously, 2) Position encodings that provide sequence information without recurrence, 3) Feed-forward networks with residual connections, 4) Layer normalization for training stability. The architecture enables parallel processing (unlike RNNs), captures long-range dependencies effectively, and scales efficiently with data and compute. This led to breakthroughs like BERT, GPT, and modern language models. The self-attention mechanism computes attention weights between all token pairs, allowing the model to understand context and relationships across the entire sequence simultaneously."
        ),

        Example(
            question="What's the difference between REST and GraphQL APIs, and when should I use each?",
            answer="REST (Representational State Transfer) and GraphQL are different API design paradigms. REST uses multiple endpoints with HTTP methods (GET, POST, PUT, DELETE) and returns fixed data structures. GraphQL uses a single endpoint with a query language that allows clients to request exactly the data they need. Key differences: 1) Data fetching: REST may require multiple requests and can over-fetch data, while GraphQL gets exactly what's requested in one query, 2) Schema: REST has implicit schemas, GraphQL has explicit, strongly-typed schemas, 3) Caching: REST benefits from HTTP caching, GraphQL caching is more complex, 4) Learning curve: REST is simpler to understand and implement. Use REST for: simple CRUD operations, when HTTP caching is crucial, microservices, or when the team is less experienced. Use GraphQL for: complex data requirements, mobile applications needing efficient data transfer, when you want to avoid over-fetching, or when you need real-time subscriptions."
        ),

        Example(
            question="How do neural networks learn through backpropagation?",
            answer="Backpropagation is the algorithm that enables neural networks to learn by efficiently computing gradients of the loss function with respect to each weight. The process works in two phases: 1) Forward pass: Input data flows through the network, each layer applies weights and activation functions, producing predictions, 2) Backward pass: The algorithm calculates how much each weight contributed to the final error by applying the chain rule of calculus. Starting from the output layer, gradients are propagated backward through each layer, computing partial derivatives that indicate how to adjust weights to minimize loss. The key insight is that gradients flow backward through the computational graph, allowing efficient calculation of all weight updates in one pass. Weights are then updated using gradient descent: weight = weight - learning_rate * gradient. This iterative process continues until the network converges to a solution that minimizes the loss function."
        ),

        Example(
            question="What are the key principles of good database design?",
            answer="Good database design follows several key principles: 1) Normalization: Organize data to reduce redundancy and improve integrity, typically aiming for 3NF (Third Normal Form) where each non-key attribute depends only on the primary key, 2) Proper indexing: Create indexes on frequently queried columns while balancing query performance with write performance, 3) Referential integrity: Use foreign keys to maintain consistent relationships between tables, 4) Appropriate data types: Choose efficient data types that accurately represent your data and optimize storage, 5) Meaningful naming conventions: Use clear, consistent names for tables, columns, and constraints, 6) Consider query patterns: Design tables and relationships based on how data will be accessed, 7) Plan for scalability: Consider future growth in data volume and user load, 8) Implement proper constraints: Use CHECK constraints, NOT NULL, and UNIQUE constraints to enforce business rules at the database level. Good design also involves understanding the trade-offs between normalization and denormalization based on specific use cases and performance requirements."
        ),

        Example(
            question="Explain the concept of Big O notation and why it matters in algorithm analysis",
            answer="Big O notation is a mathematical notation used to describe the upper bound of an algorithm's time or space complexity as input size approaches infinity. It focuses on the dominant term and ignores constants and lower-order terms. Common complexities include: O(1) constant time, O(log n) logarithmic, O(n) linear, O(n log n) linearithmic, O(n²) quadratic, and O(2^n) exponential. Big O matters because: 1) It helps predict how algorithms scale with larger inputs, 2) Enables comparison between different algorithms solving the same problem, 3) Guides optimization decisions by identifying bottlenecks, 4) Is crucial for system design and performance planning. For example, a O(n²) algorithm might work fine for 100 items but become unusable for 10,000 items, while a O(n log n) algorithm remains efficient. Understanding Big O helps developers choose appropriate data structures and algorithms, optimize code performance, and design systems that scale effectively with growing data and user bases."
        ),

        Example(
            question="How does the event loop work in JavaScript, and why is it important?",
            answer="The JavaScript event loop is the mechanism that handles asynchronous operations in JavaScript's single-threaded environment. It works through several components: 1) Call Stack: Executes synchronous code in LIFO order, 2) Web APIs: Handle asynchronous operations like setTimeout, DOM events, and HTTP requests, 3) Callback Queue: Stores callbacks from completed async operations, 4) Event Loop: Continuously checks if the call stack is empty and moves callbacks from the queue to the stack. The process: when an async operation completes, its callback goes to the callback queue. The event loop waits for the call stack to be empty, then moves the first callback from the queue to the stack for execution. This mechanism is crucial because: it prevents blocking the main thread, enables non-blocking I/O operations, allows JavaScript to handle multiple operations concurrently despite being single-threaded, and ensures responsive user interfaces. Understanding the event loop helps developers write efficient async code, avoid blocking operations, and debug timing-related issues in JavaScript applications."
        ),

        Example(
            question="What are the principles of effective user interface design?",
            answer="Effective UI design follows several key principles: 1) Clarity: Interface elements should be immediately understandable, with clear visual hierarchy and intuitive navigation, 2) Consistency: Use consistent patterns, colors, typography, and interactions throughout the application, 3) Feedback: Provide immediate visual/audio feedback for user actions, including loading states, error messages, and success confirmations, 4) Accessibility: Design for users with disabilities by following WCAG guidelines, providing alt text, keyboard navigation, and proper color contrast, 5) Simplicity: Remove unnecessary elements and focus on essential functionality - every element should serve a purpose, 6) User Control: Allow users to undo actions, customize their experience, and feel in control of the interface, 7) Recognition over Recall: Make options visible rather than requiring users to remember information, 8) Error Prevention: Design to prevent errors through constraints, confirmations, and clear instructions, 9) Efficiency: Enable both novice and expert users to accomplish tasks quickly, 10) Aesthetic Appeal: Balance functionality with visual appeal to create engaging experiences. These principles work together to create interfaces that are not only functional but also enjoyable and inclusive for all users."
        ),

        Example(
            question="How does distributed system consistency work, and what are the trade-offs?",
            answer="Distributed system consistency refers to how data remains coherent across multiple nodes in a distributed system. The CAP theorem states you can only guarantee two of: Consistency (all nodes see the same data simultaneously), Availability (system remains operational), and Partition tolerance (system continues despite network failures). Consistency models include: 1) Strong consistency: All nodes see the same data at the same time, requiring coordination and potentially sacrificing availability, 2) Eventual consistency: System will become consistent over time, allowing for temporary inconsistencies, 3) Weak consistency: No guarantees about when data will be consistent across nodes. Trade-offs involve: Performance vs. Consistency (stronger consistency often requires more coordination overhead), Availability vs. Consistency (maintaining consistency may require taking nodes offline), Complexity vs. Guarantees (stronger consistency models increase system complexity). Real-world examples: Banking systems typically choose strong consistency for account balances, social media platforms often use eventual consistency for posts and likes, and DNS systems use eventual consistency for global propagation. Understanding these trade-offs helps architects design systems that balance business requirements with technical constraints."
        ),

        Example(
            question="What is the difference between machine learning, deep learning, and artificial intelligence?",
            answer="AI, Machine Learning, and Deep Learning are related but distinct concepts forming a hierarchical relationship. Artificial Intelligence is the broadest field, encompassing any technique that enables machines to mimic human intelligence, including rule-based systems, expert systems, and learning algorithms. Machine Learning is a subset of AI that focuses on algorithms that improve performance through experience without being explicitly programmed for every scenario. It includes supervised learning (learning from labeled data), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning through trial and error). Deep Learning is a subset of ML that uses neural networks with multiple layers (typically 3+ hidden layers) to learn complex patterns. Key differences: 1) Scope: AI > ML > DL, 2) Data requirements: Deep learning typically needs more data than traditional ML, 3) Interpretability: Traditional ML models are often more interpretable than deep learning models, 4) Computational requirements: Deep learning requires more computational power, 5) Feature engineering: Traditional ML often requires manual feature engineering, while deep learning can automatically learn features. Examples: AI includes chess programs, ML includes recommendation systems, and DL includes image recognition and language models."
        ),

        Example(
            question="How do you design a system to handle millions of concurrent users?",
            answer="Designing for millions of concurrent users requires a multi-layered approach addressing scalability, performance, and reliability. Key strategies include: 1) Horizontal scaling: Use load balancers to distribute requests across multiple servers, implement auto-scaling groups that add/remove instances based on demand, 2) Caching strategies: Implement CDNs for static content, use Redis/Memcached for application-level caching, implement database query result caching, 3) Database optimization: Use read replicas to distribute read load, implement database sharding to partition data across multiple databases, consider NoSQL databases for specific use cases, 4) Asynchronous processing: Use message queues (RabbitMQ, Apache Kafka) for background tasks, implement event-driven architecture to decouple services, 5) Microservices architecture: Break monolithic applications into smaller, independently scalable services, use API gateways for service communication and rate limiting, 6) Performance optimization: Implement efficient algorithms and data structures, use connection pooling, enable compression, optimize images and assets, 7) Monitoring and observability: Implement comprehensive logging, metrics, and alerting systems, use distributed tracing to monitor request flows. Success requires careful planning, gradual scaling, extensive testing, and continuous monitoring to identify and address bottlenecks before they impact users."
        ),

        Example(
            question="What are the key considerations for implementing cybersecurity in modern applications?",
            answer="Modern application cybersecurity requires a comprehensive, multi-layered approach addressing various attack vectors and compliance requirements. Key considerations include: 1) Authentication and Authorization: Implement multi-factor authentication, use OAuth 2.0/OpenID Connect for secure token-based auth, apply principle of least privilege, regularly audit user permissions, 2) Data Protection: Encrypt data at rest and in transit using strong encryption (AES-256), implement proper key management, hash passwords with bcrypt or similar, 3) Input Validation: Sanitize all user inputs to prevent injection attacks, validate data types and ranges, use parameterized queries for database interactions, 4) Secure Communication: Use HTTPS everywhere, implement proper TLS configuration, use security headers (HSTS, CSP, X-Frame-Options), 5) Vulnerability Management: Regularly update dependencies, conduct security audits and penetration testing, implement automated security scanning in CI/CD pipelines, 6) Monitoring and Incident Response: Log security events, monitor for suspicious activity, have incident response plans, implement rate limiting and DDoS protection, 7) Compliance: Follow relevant standards (GDPR, HIPAA, PCI-DSS), implement proper data retention policies, ensure audit trails. Security should be integrated throughout the development lifecycle, not added as an afterthought."
        )
    ]

    optimize_program(MentorAgent, trainset=trainset)

